{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b5f507",
   "metadata": {
    "id": "UUv9Dfir60vr",
    "papermill": {
     "duration": 0.006088,
     "end_time": "2023-11-01T13:08:33.823870",
     "exception": false,
     "start_time": "2023-11-01T13:08:33.817782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TP3 - Deep Learning Models\n",
    "\n",
    "In the previous notebooks, we used classic machine learning models based on features computed on the data. Now, we will use Deep Learning to perform sleep staging directly on the raw signals. \n",
    "\n",
    "In this tutorial we will use pytorch: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fbf252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:33.838901Z",
     "iopub.status.busy": "2023-11-01T13:08:33.838075Z",
     "iopub.status.idle": "2023-11-01T13:08:33.849739Z",
     "shell.execute_reply": "2023-11-01T13:08:33.848851Z"
    },
    "papermill": {
     "duration": 0.022763,
     "end_time": "2023-11-01T13:08:33.852325",
     "exception": false,
     "start_time": "2023-11-01T13:08:33.829562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/automated-sleep-staging-beacon-biosignals-2023-2024\n"
     ]
    }
   ],
   "source": [
    "cd \"/kaggle/input/automated-sleep-staging-beacon-biosignals-2023-2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d72d7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:33.866089Z",
     "iopub.status.busy": "2023-11-01T13:08:33.865623Z",
     "iopub.status.idle": "2023-11-01T13:08:34.260909Z",
     "shell.execute_reply": "2023-11-01T13:08:34.259660Z"
    },
    "papermill": {
     "duration": 0.405991,
     "end_time": "2023-11-01T13:08:34.264434",
     "exception": false,
     "start_time": "2023-11-01T13:08:33.858443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b996f8",
   "metadata": {
    "papermill": {
     "duration": 0.005707,
     "end_time": "2023-11-01T13:08:34.276277",
     "exception": false,
     "start_time": "2023-11-01T13:08:34.270570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "First, we load the data from the first EEG channel for each record and the associated hypnogram.\n",
    "\n",
    "Then, we sample the training and validation records for the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5140b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:34.290173Z",
     "iopub.status.busy": "2023-11-01T13:08:34.289526Z",
     "iopub.status.idle": "2023-11-01T13:08:50.112760Z",
     "shell.execute_reply": "2023-11-01T13:08:50.111617Z"
    },
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1603111976603,
     "user": {
      "displayName": "Antoine Guillot",
      "photoUrl": "",
      "userId": "13673594054995143914"
     },
     "user_tz": -120
    },
    "id": "iG_vEjt_60vs",
    "papermill": {
     "duration": 15.833788,
     "end_time": "2023-11-01T13:08:50.116041",
     "exception": false,
     "start_time": "2023-11-01T13:08:34.282253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_records = {}\n",
    "hypnogram_for_records = {}\n",
    "hypnograms = pd.read_csv('targets_train.csv')\n",
    "for record in os.listdir(\"training_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    x = np.load(f'training_records/{record}')\n",
    "    data_for_records[record] = x[:,1:250 * 30 + 1]\n",
    "    hypnogram_for_records[record] = list(hypnograms[hypnograms['record'] == record_number]['target'])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfeb0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:50.129817Z",
     "iopub.status.busy": "2023-11-01T13:08:50.129279Z",
     "iopub.status.idle": "2023-11-01T13:08:50.136160Z",
     "shell.execute_reply": "2023-11-01T13:08:50.134915Z"
    },
    "papermill": {
     "duration": 0.017339,
     "end_time": "2023-11-01T13:08:50.139640",
     "exception": false,
     "start_time": "2023-11-01T13:08:50.122301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training records:  ['dreem_4.npy', 'dreem_2.npy', 'dreem_6.npy', 'dreem_5.npy', 'dreem_1.npy']\n",
      "Test records:  ['dreem_3.npy', 'dreem_0.npy']\n"
     ]
    }
   ],
   "source": [
    "rd.seed(1234)\n",
    "records_list = list(data_for_records)\n",
    "rd.shuffle(records_list)\n",
    "training_record,test_records = records_list[:5],records_list[5:]\n",
    "\n",
    "print('Training records: ',training_record)\n",
    "print('Test records: ', test_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5184b1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:50.153874Z",
     "iopub.status.busy": "2023-11-01T13:08:50.153413Z",
     "iopub.status.idle": "2023-11-01T13:08:50.405379Z",
     "shell.execute_reply": "2023-11-01T13:08:50.404108Z"
    },
    "papermill": {
     "duration": 0.263058,
     "end_time": "2023-11-01T13:08:50.408542",
     "exception": false,
     "start_time": "2023-11-01T13:08:50.145484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(records, data_for_records,hypnogram_for_records):\n",
    "    X,y = [],[]\n",
    "    for record in records:\n",
    "        X.append(data_for_records[record])\n",
    "        y.extend(hypnogram_for_records[record])\n",
    "\n",
    "    return np.concatenate(X),y\n",
    "\n",
    "\n",
    "X_train,y_train = build_dataset(training_record,data_for_records,hypnogram_for_records)\n",
    "X_test,y_test = build_dataset(test_records,data_for_records,hypnogram_for_records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc13417",
   "metadata": {
    "id": "dxnqHGci60v0",
    "papermill": {
     "duration": 0.005678,
     "end_time": "2023-11-01T13:08:50.420180",
     "exception": false,
     "start_time": "2023-11-01T13:08:50.414502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataloader and dataset\n",
    "\n",
    "In this TD, we will only work with one EEG channel.\n",
    "Let's create dataset functions that will be used for training and testing the model:\n",
    "\n",
    "*EegEpochDataset*: Eeg Class herited from pytorch Dataset to deal with our data\n",
    "\n",
    "The dataloader are used by pytorch to randomly sampled elements from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bcafe5",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:50.434519Z",
     "iopub.status.busy": "2023-11-01T13:08:50.434084Z",
     "iopub.status.idle": "2023-11-01T13:08:54.968300Z",
     "shell.execute_reply": "2023-11-01T13:08:54.966915Z"
    },
    "executionInfo": {
     "elapsed": 3754,
     "status": "error",
     "timestamp": 1603111979611,
     "user": {
      "displayName": "Antoine Guillot",
      "photoUrl": "",
      "userId": "13673594054995143914"
     },
     "user_tz": -120
    },
    "id": "CoIGS2qZ60v1",
    "outputId": "f5ee4748-ceac-4ae8-8973-4738b0bd91fc",
    "papermill": {
     "duration": 4.545444,
     "end_time": "2023-11-01T13:08:54.971545",
     "exception": false,
     "start_time": "2023-11-01T13:08:50.426101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Load project data\n",
    "    DataLoader and Dataset for single-channel EEG\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def normalize_data(eeg_array):\n",
    "    \"\"\"normalize signal between 0 and 1\"\"\"\n",
    "\n",
    "    normalized_array = np.clip(eeg_array, -250, 250)\n",
    "    normalized_array = normalized_array / 250\n",
    "\n",
    "    return normalized_array\n",
    "\n",
    "\n",
    "class EegEpochDataset(Dataset):\n",
    "    \"\"\"EEG Epochs dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_data (numpy array): Numpy array of input data.\n",
    "            y_data (list of numpy array): Sleep Stages\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.y_data = y_data\n",
    "        self.x_data = x_data\n",
    "        self.transform = transform\n",
    "\n",
    "        self.x_data = normalize_data(x_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        signal = np.expand_dims(self.x_data[idx], axis=0)\n",
    "        stage = self.y_data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return signal, stage\n",
    "\n",
    "\n",
    "training_dataset = EegEpochDataset(X_train,y_train)\n",
    "training_dataloader = DataLoader(training_dataset,batch_size = 32)\n",
    "validation_dataset = EegEpochDataset(X_test,y_test)\n",
    "validation_dataloader = DataLoader(validation_dataset,batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba1c6",
   "metadata": {
    "id": "SEV2YgRT60v7",
    "papermill": {
     "duration": 0.005606,
     "end_time": "2023-11-01T13:08:54.983821",
     "exception": false,
     "start_time": "2023-11-01T13:08:54.978215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## First CNN model\n",
    "The model is rather simple, it's a succession of convolution layers with non-linearities.\n",
    "\n",
    "Then, we apply max-pooling followed by a classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76946eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:54.998139Z",
     "iopub.status.busy": "2023-11-01T13:08:54.997502Z",
     "iopub.status.idle": "2023-11-01T13:08:55.006986Z",
     "shell.execute_reply": "2023-11-01T13:08:55.005711Z"
    },
    "executionInfo": {
     "elapsed": 3744,
     "status": "aborted",
     "timestamp": 1603111979603,
     "user": {
      "displayName": "Antoine Guillot",
      "photoUrl": "",
      "userId": "13673594054995143914"
     },
     "user_tz": -120
    },
    "id": "-Gkm01M_60v8",
    "papermill": {
     "duration": 0.019625,
     "end_time": "2023-11-01T13:08:55.009533",
     "exception": false,
     "start_time": "2023-11-01T13:08:54.989908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SingleChannelConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SingleChannelConvNet, self).__init__()\n",
    "        self.conv_a = nn.Conv1d(1, 8, 25, stride=5)\n",
    "        self.conv_b = nn.Conv1d(8, 16, 10, stride=5)\n",
    "        self.conv_c = nn.Conv1d(16, 32, 10, stride=5)\n",
    "        self.conv_d = nn.Conv1d(32, 64, 10, stride=5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.conv_a(x))\n",
    "        x = self.relu(self.conv_b(x))\n",
    "        x = self.relu(self.conv_c(x))\n",
    "        x = self.relu(self.conv_d(x))\n",
    "        x = x.max(-1)[0]\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c0fa4",
   "metadata": {
    "papermill": {
     "duration": 0.006074,
     "end_time": "2023-11-01T13:08:55.021725",
     "exception": false,
     "start_time": "2023-11-01T13:08:55.015651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Question:** \n",
    "- What is the downsampling rate of the network ? \n",
    "- What does the following line do ?\n",
    ">x = x.max(-1)[0]\n",
    "- How many parameters are there in this network ? Is it a lot ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbb94a",
   "metadata": {
    "id": "9P6xkBIq60wB",
    "papermill": {
     "duration": 0.005473,
     "end_time": "2023-11-01T13:08:55.033140",
     "exception": false,
     "start_time": "2023-11-01T13:08:55.027667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Training Loop\n",
    "We iterate on the training dataloader and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1f7394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:08:55.047653Z",
     "iopub.status.busy": "2023-11-01T13:08:55.047212Z",
     "iopub.status.idle": "2023-11-01T13:10:01.590702Z",
     "shell.execute_reply": "2023-11-01T13:10:01.589322Z"
    },
    "executionInfo": {
     "elapsed": 3742,
     "status": "aborted",
     "timestamp": 1603111979606,
     "user": {
      "displayName": "Antoine Guillot",
      "photoUrl": "",
      "userId": "13673594054995143914"
     },
     "user_tz": -120
    },
    "id": "lk5zPOOs60wC",
    "papermill": {
     "duration": 66.554512,
     "end_time": "2023-11-01T13:10:01.593736",
     "exception": false,
     "start_time": "2023-11-01T13:08:55.039224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "epoch 1, 4288 samples, loss: 1.494\n",
      "epoch 2, 4288 samples, loss: 1.406\n",
      "epoch 3, 4288 samples, loss: 1.391\n",
      "epoch 4, 4288 samples, loss: 1.376\n",
      "epoch 5, 4288 samples, loss: 1.360\n",
      "epoch 6, 4288 samples, loss: 1.320\n",
      "epoch 7, 4288 samples, loss: 1.239\n",
      "epoch 8, 4288 samples, loss: 1.135\n",
      "epoch 9, 4288 samples, loss: 1.052\n",
      "epoch 10, 4288 samples, loss: 1.000\n",
      "epoch 11, 4288 samples, loss: 0.967\n",
      "epoch 12, 4288 samples, loss: 0.942\n",
      "epoch 13, 4288 samples, loss: 0.920\n",
      "epoch 14, 4288 samples, loss: 0.901\n",
      "epoch 15, 4288 samples, loss: 0.878\n",
      "epoch 16, 4288 samples, loss: 0.856\n",
      "epoch 17, 4288 samples, loss: 0.832\n",
      "epoch 18, 4288 samples, loss: 0.806\n",
      "epoch 19, 4288 samples, loss: 0.781\n",
      "epoch 20, 4288 samples, loss: 0.754\n",
      "epoch 21, 4288 samples, loss: 0.728\n",
      "epoch 22, 4288 samples, loss: 0.702\n",
      "epoch 23, 4288 samples, loss: 0.676\n",
      "epoch 24, 4288 samples, loss: 0.650\n",
      "epoch 25, 4288 samples, loss: 0.620\n",
      "epoch 26, 4288 samples, loss: 0.595\n",
      "epoch 27, 4288 samples, loss: 0.565\n",
      "epoch 28, 4288 samples, loss: 0.538\n",
      "epoch 29, 4288 samples, loss: 0.512\n",
      "epoch 30, 4288 samples, loss: 0.485\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# device: use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parameters\n",
    "n_epoch = 30\n",
    "\n",
    "# neural network and co\n",
    "my_net = SingleChannelConvNet()\n",
    "my_net = my_net.to(device) # model into GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(my_net.parameters())\n",
    "my_net.train()\n",
    "print('training...')\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(training_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = my_net.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('epoch %d, %d samples, loss: %.3f' % (epoch + 1, (i+1)*training_dataloader.batch_size,running_loss / (i+1)))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f2e24",
   "metadata": {
    "papermill": {
     "duration": 0.007243,
     "end_time": "2023-11-01T13:10:01.608721",
     "exception": false,
     "start_time": "2023-11-01T13:10:01.601478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Assessing the model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee1608",
   "metadata": {
    "id": "mrotrqI860wI",
    "papermill": {
     "duration": 0.007347,
     "end_time": "2023-11-01T13:10:01.623735",
     "exception": false,
     "start_time": "2023-11-01T13:10:01.616388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the training is complete, let's assess its performance on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6994986c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:10:01.641266Z",
     "iopub.status.busy": "2023-11-01T13:10:01.640597Z",
     "iopub.status.idle": "2023-11-01T13:10:03.377806Z",
     "shell.execute_reply": "2023-11-01T13:10:03.376571Z"
    },
    "executionInfo": {
     "elapsed": 3739,
     "status": "aborted",
     "timestamp": 1603111979609,
     "user": {
      "displayName": "Antoine Guillot",
      "photoUrl": "",
      "userId": "13673594054995143914"
     },
     "user_tz": -120
    },
    "id": "4OPeYIjs60wK",
    "papermill": {
     "duration": 1.748958,
     "end_time": "2023-11-01T13:10:03.380326",
     "exception": false,
     "start_time": "2023-11-01T13:10:01.631368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_accuracy\n",
      "0.47550950430996863\n",
      "cohen_kappa_score\n",
      "0.4999046450340985\n",
      "confusion_matrix\n",
      "[[ 17   1  29  20  43]\n",
      " [  2   4  30   4  27]\n",
      " [  3  10 481  36 184]\n",
      " [  5   1  61 336   5]\n",
      " [ 22   1  98  76 393]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "# params\n",
    "classes = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_list = torch.empty(0).to(device)\n",
    "    true_list = torch.empty(0).to(device)\n",
    "    for data in validation_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "        \n",
    "        outputs = my_net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_list = torch.cat([prediction_list, predicted])\n",
    "        true_list = torch.cat([true_list, labels])\n",
    "\n",
    "true_list = true_list.cpu().numpy()\n",
    "prediction_list = prediction_list.cpu().numpy()\n",
    "scores = {'balanced_accuracy': balanced_accuracy_score(true_list, prediction_list),\n",
    "            'cohen_kappa_score': cohen_kappa_score(true_list, prediction_list),\n",
    "            'confusion_matrix': confusion_matrix(true_list, prediction_list)}\n",
    "\n",
    "for elt in scores:\n",
    "    print(elt)\n",
    "    print(scores[elt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282d64e",
   "metadata": {
    "papermill": {
     "duration": 0.00795,
     "end_time": "2023-11-01T13:10:03.396272",
     "exception": false,
     "start_time": "2023-11-01T13:10:03.388322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Question:**\n",
    "- How does the CNN ranks compared to model from the previous tutorial ? How could you improve it ?\n",
    "- Where do the mistakes happen ? Plot a few epoch to understand when the network is good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7ee1a",
   "metadata": {
    "papermill": {
     "duration": 0.007322,
     "end_time": "2023-11-01T13:10:03.411344",
     "exception": false,
     "start_time": "2023-11-01T13:10:03.404022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting started with Deep Learning:\n",
    "    \n",
    "- Pytorch Blitz: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "- Pytorch Documentation: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3065e",
   "metadata": {
    "papermill": {
     "duration": 0.007308,
     "end_time": "2023-11-01T13:10:03.426342",
     "exception": false,
     "start_time": "2023-11-01T13:10:03.419034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d89b2",
   "metadata": {
    "papermill": {
     "duration": 0.007259,
     "end_time": "2023-11-01T13:10:03.441206",
     "exception": false,
     "start_time": "2023-11-01T13:10:03.433947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Question 1: The current model is trained without early stopping, split the records between the training set, validation set and test set and use early stopping to stop the training at the right moment.**\n",
    "\n",
    "**Question 2: The model parameters are not optimized, by building a k-folds validation optimize the model hyperparameters**\n",
    "\n",
    "**Question 3: As we have seen in the previous tutorial, spectrogram representation may be an efficient way to represent sleep. Build a model (CNN or RNN) which takes the spectrogram of the data as input. You can use the spectrogram from https://pytorch.org/audio/stable/transforms.html**\n",
    "\n",
    "**Question 4: The current model only use a single EEG channel, what is the impact of using all the EEG channels ?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4eb70",
   "metadata": {
    "papermill": {
     "duration": 0.007549,
     "end_time": "2023-11-01T13:10:03.456922",
     "exception": false,
     "start_time": "2023-11-01T13:10:03.449373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Additional challenge:** \n",
    "\n",
    "If you are done with sleep staging classification, you can try this other challenge on physiological data. The goal is to segment sleep apnea events based on PSG data.\n",
    "https://challengedata.ens.fr/challenges/45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 95.572258,
   "end_time": "2023-11-01T13:10:05.775676",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T13:08:30.203418",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
